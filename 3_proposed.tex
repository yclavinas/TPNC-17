\section{GA-BBOB}\label{sec:proposed:ga-bbob}

In the work, we propose the GA-BBOB, which is a real-valued Genetic Algorithm to explore the search space of the 24 noise free 40-D BBOB benchmark functions. In some aspects the GA-BBOB implementation is equal to the GAModel. That is mainly true for all evolutionary operators. The search space of the benchmark functions are distinct from the GAModel search space, we reformulated the Genome Representation and the Fitness Function.

\subsubsection*{Genome Representation and Evolutionary Tools.}
Each individual is represented as real valued array, where each element is one input to a 40-D noise free benchmark BBOB function. Therefore, each individual of the GA-BBOB has size of 40 real value elements. Each element's values has values between the interval [-5, 5], once those function are limited by these values. As in the GAModel it is used the Two-Point crossover, elitism and polynomial bounded mutation are used as evolutionary operators. The relevant parameters were set as Elite Size = 1, Crossover chance = 0.9, Mutation Chance = 0.1. For the Polynomial Bounded parameters, a small modification, when comparing to the GAModel, was made: eta = 1, low = -5, up = 5.


\subsubsection*{Fitness Function.}
The fitness function considered are the 40-D noise free benchmark BBOB function. For more information, please refer to the work of Hansen et al.~\cite{hansen2010real}.
Consequently, the GA-BBOB has 24 different fitness functions. 


%TODO:
%add here the PRCGA and the restart and number of eval stuff
\subsubsection*{Minor Adaptations.}
%check if this is the most current work on ga and bbob
Chuang et al.~\cite{chuang2012black}, proposed a Genetic Algorithm specific to run on the noise free BBOB benchmark functions. Some good practices presented in this work were incorporated to the GAModel and to the GA-BBOB. These practice are the concept of ``restart strategy'' and the concept of ``alleviate stagnation''.

The restart strategy concept is defined by Chuang et al. as: ``For each restart, the initial population  is uniformly and randomly sampled within the search space. Whenever the restart condition is met, the algorithm will be reinitialized without using any information about the last test run. This process is iterated until the stopping criteria are met, i.e., maximum number of function evaluations has been reached, or the function value is less than the target precision.''

The restart condition used for both GAModel and GA-BBOB is that the standard deviation of the current population is smaller that $10^{-12}$. When the restart condition is contemplated the algorithm population is reinitialized randomly, as in the first initialization. In the GAModel and in the GA-BBOB, we consider information about the last run, by utilizing the Elitism strategy on the best individual prior to the restart. The maximum number of function evaluations chosen is ($10^5$) and the target precision chosen is ($10^{-8}$), values compatible with the ones in the previous work.

\subsection{Genetic Algorithms and Benchmarks Functions}

IN 2009, Nicolau~\cite{nicolau2009application} proposed the first Genetic Algorithm on the BBOB-2009 noiseless testbed. It was a simple binary Genetic Algorithm and its results showed good results on separable functions, but poor performance is achieved on the other functions.

In 2012, Chuang et al.~\cite{chuang2012black} presented the ``DBRCGA'', a real coded Genetic Algorithm that uses relative fitness information to direct the crossover toward a direction that significantly improves the objective fitness. The DBRCGA was tested on the BBOB-2012 noiseless testbed. Their results showed that the DBRCGA performs with difficulty in getting a solution with the desired accuracy for high conditioning and multimodal functions within the specified maximum number of evaluations, the DBRCGA presents good performance in separable function and functions with low or moderate conditioning.

In 2013, Holtschulte et al.~\cite{holtschulte2013benchmarking} evaluated two Genetic Algorithm on the BBOB-2013 noiseless testbed, but none had results competitive with the best 2009 optimization algorithm. Their results highlight the importance of carefully chosen genetic operators.

Sawyerr et al found simliar results to the one achieved by Chuang et al. on their works in 2013~\cite{sawyerr2013benchmarking} and in 2015~\cite{sawyerr2015benchmarking}. In 2013, they proposed the ``PRCGA'', or projection-based real-coded genetic algorithm. It incorporates exploratory search mecanism based on vector projection. In 2015, they have studied a hybrid application of the Genetic Algorithm (GA) with the \textit{uniform random direction} search, named ``RCGAu''. BOth the PRCGA and the RCGAu were tested on the BBOB-2013 noiseless testbed. They stated that both of them had difficulties to achieve solutions with the desired accuracy for high conditioning and multimodal functions, though they were able to solve the benchmark functions.  Sawyerr et al concluded that the RCGAu has excelled in solving the $f_1$, $f_2$, $f_3$, $f_7$ and $f_{21}$, tough for the other functions it achieved avarage results. They also state that real value GA do not efficiently solve highly conditioned problems and studies have currently been carried out to find out why~\cite{sawyerr2015benchmarking}.
